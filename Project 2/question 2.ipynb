{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a543187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for creating regular densely-connected NN layers.\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7db78fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>OUTPUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A  B  C  D  E  OUTPUT\n",
       "0   1  1  1  0  0       1\n",
       "1   1  1  1  0  1       1\n",
       "2   0  0  0  1  0       0\n",
       "3   0  0  0  1  1       0\n",
       "4   0  0  1  0  0       1\n",
       "5   0  0  1  0  1       1\n",
       "6   0  0  1  1  0       1\n",
       "7   0  0  1  1  1       1\n",
       "8   0  1  0  0  0       0\n",
       "9   0  1  0  0  1       0\n",
       "10  0  1  0  1  0       0\n",
       "11  0  1  0  1  1       0\n",
       "12  0  1  1  0  0       1\n",
       "13  0  1  1  0  1       1\n",
       "14  0  1  1  1  0       1\n",
       "15  0  1  1  1  1       1\n",
       "16  1  0  0  0  0       1\n",
       "17  1  0  0  0  1       1\n",
       "18  1  0  0  1  0       1\n",
       "19  1  0  0  1  1       1\n",
       "20  1  0  1  0  0       1\n",
       "21  1  0  1  0  1       1\n",
       "22  1  0  1  1  0       1\n",
       "23  1  0  1  1  1       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reading the\n",
    "training = pd.read_csv(r'training.csv', header = 0)\n",
    "display(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941362bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training set into data and target\n",
    "training_arr = training.values\n",
    "data_input = training_arr[:,0:5]\n",
    "data_output = training_arr[:,5]\n",
    "\n",
    "# Splitting the data into a training set and a test set\n",
    "testing_set_size = 0.2\n",
    "\n",
    "input_train, input_test, output_train, output_test = train_test_split(\n",
    "data_input, data_output, test_size=testing_set_size, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7454aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the keras model with 2 hidden layers with 3 and 2 nodes on each\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_shape=(5,), activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0498063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 206ms/step - loss: 0.5567 - accuracy: 0.7333 - val_loss: 0.4134 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5541 - accuracy: 0.7333 - val_loss: 0.4110 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5520 - accuracy: 0.7333 - val_loss: 0.4096 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5502 - accuracy: 0.7333 - val_loss: 0.4076 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5482 - accuracy: 0.7333 - val_loss: 0.4051 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5472 - accuracy: 0.7333 - val_loss: 0.4017 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5452 - accuracy: 0.7333 - val_loss: 0.3993 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5437 - accuracy: 0.7333 - val_loss: 0.3966 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5426 - accuracy: 0.7333 - val_loss: 0.3936 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5409 - accuracy: 0.7333 - val_loss: 0.3912 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5394 - accuracy: 0.7333 - val_loss: 0.3892 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5382 - accuracy: 0.7333 - val_loss: 0.3868 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5368 - accuracy: 0.7333 - val_loss: 0.3847 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5353 - accuracy: 0.7333 - val_loss: 0.3827 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5339 - accuracy: 0.7333 - val_loss: 0.3808 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5327 - accuracy: 0.7333 - val_loss: 0.3783 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5313 - accuracy: 0.7333 - val_loss: 0.3760 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5299 - accuracy: 0.7333 - val_loss: 0.3739 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5287 - accuracy: 0.7333 - val_loss: 0.3715 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5273 - accuracy: 0.7333 - val_loss: 0.3695 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5260 - accuracy: 0.7333 - val_loss: 0.3670 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5247 - accuracy: 0.7333 - val_loss: 0.3650 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5235 - accuracy: 0.7333 - val_loss: 0.3630 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5222 - accuracy: 0.7333 - val_loss: 0.3613 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5210 - accuracy: 0.7333 - val_loss: 0.3597 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5198 - accuracy: 0.7333 - val_loss: 0.3573 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5184 - accuracy: 0.7333 - val_loss: 0.3555 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5174 - accuracy: 0.7333 - val_loss: 0.3534 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5161 - accuracy: 0.7333 - val_loss: 0.3513 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5147 - accuracy: 0.7333 - val_loss: 0.3494 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5136 - accuracy: 0.7333 - val_loss: 0.3477 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5124 - accuracy: 0.7333 - val_loss: 0.3458 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5113 - accuracy: 0.7333 - val_loss: 0.3441 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5102 - accuracy: 0.7333 - val_loss: 0.3431 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5089 - accuracy: 0.7333 - val_loss: 0.3411 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5078 - accuracy: 0.7333 - val_loss: 0.3394 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5065 - accuracy: 0.7333 - val_loss: 0.3378 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5054 - accuracy: 0.7333 - val_loss: 0.3361 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5042 - accuracy: 0.7333 - val_loss: 0.3345 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5030 - accuracy: 0.7333 - val_loss: 0.3331 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5020 - accuracy: 0.7333 - val_loss: 0.3315 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5010 - accuracy: 0.7333 - val_loss: 0.3301 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5000 - accuracy: 0.7333 - val_loss: 0.3287 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4990 - accuracy: 0.7333 - val_loss: 0.3272 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4978 - accuracy: 0.7333 - val_loss: 0.3262 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4970 - accuracy: 0.7333 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4959 - accuracy: 0.7333 - val_loss: 0.3235 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4949 - accuracy: 0.7333 - val_loss: 0.3223 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4940 - accuracy: 0.7333 - val_loss: 0.3208 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4930 - accuracy: 0.7333 - val_loss: 0.3197 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4922 - accuracy: 0.7333 - val_loss: 0.3182 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4912 - accuracy: 0.7333 - val_loss: 0.3173 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4902 - accuracy: 0.7333 - val_loss: 0.3160 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4894 - accuracy: 0.7333 - val_loss: 0.3146 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4884 - accuracy: 0.7333 - val_loss: 0.3133 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4874 - accuracy: 0.7333 - val_loss: 0.3122 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4864 - accuracy: 0.7333 - val_loss: 0.3113 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4855 - accuracy: 0.7333 - val_loss: 0.3100 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4847 - accuracy: 0.7333 - val_loss: 0.3092 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4838 - accuracy: 0.7333 - val_loss: 0.3078 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4828 - accuracy: 0.7333 - val_loss: 0.3067 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4820 - accuracy: 0.7333 - val_loss: 0.3058 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4811 - accuracy: 0.7333 - val_loss: 0.3048 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4802 - accuracy: 0.7333 - val_loss: 0.3034 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4793 - accuracy: 0.7333 - val_loss: 0.3025 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4783 - accuracy: 0.7333 - val_loss: 0.3014 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4775 - accuracy: 0.7333 - val_loss: 0.3002 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4766 - accuracy: 0.7333 - val_loss: 0.2987 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4757 - accuracy: 0.7333 - val_loss: 0.2973 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4747 - accuracy: 0.7333 - val_loss: 0.2963 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4739 - accuracy: 0.7333 - val_loss: 0.2957 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4729 - accuracy: 0.7333 - val_loss: 0.2948 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4722 - accuracy: 0.7333 - val_loss: 0.2934 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4712 - accuracy: 0.7333 - val_loss: 0.2930 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4702 - accuracy: 0.7333 - val_loss: 0.2920 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4693 - accuracy: 0.7333 - val_loss: 0.2909 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4685 - accuracy: 0.7333 - val_loss: 0.2896 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4675 - accuracy: 0.7333 - val_loss: 0.2888 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4665 - accuracy: 0.7333 - val_loss: 0.2880 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4656 - accuracy: 0.7333 - val_loss: 0.2870 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4649 - accuracy: 0.7333 - val_loss: 0.2860 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4639 - accuracy: 0.7333 - val_loss: 0.2849 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4632 - accuracy: 0.7333 - val_loss: 0.2845 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4621 - accuracy: 0.7333 - val_loss: 0.2836 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4612 - accuracy: 0.7333 - val_loss: 0.2825 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4604 - accuracy: 0.7333 - val_loss: 0.2814 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4594 - accuracy: 0.7333 - val_loss: 0.2804 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4585 - accuracy: 0.7333 - val_loss: 0.2796 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4575 - accuracy: 0.7333 - val_loss: 0.2787 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4566 - accuracy: 0.7333 - val_loss: 0.2775 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4555 - accuracy: 0.7333 - val_loss: 0.2766 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4547 - accuracy: 0.7333 - val_loss: 0.2760 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4538 - accuracy: 0.7333 - val_loss: 0.2748 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4528 - accuracy: 0.7333 - val_loss: 0.2737 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4522 - accuracy: 0.7333 - val_loss: 0.2735 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4510 - accuracy: 0.7333 - val_loss: 0.2726 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4500 - accuracy: 0.7333 - val_loss: 0.2716 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4494 - accuracy: 0.7333 - val_loss: 0.2712 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4482 - accuracy: 0.7333 - val_loss: 0.2701 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4473 - accuracy: 0.7333 - val_loss: 0.2692 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7257 - accuracy: 0.6000\n",
      "Accuracy: 60.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fitting the model to the dataset we got\n",
    "history = model.fit(input_train, output_train, validation_split=0.2, epochs=100,batch_size=5)\n",
    "\n",
    "# evaluate the model and print out the accuracy \n",
    "_, accuracy = model.evaluate(input_test, output_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43044fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d74077b820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aeb27384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>OUTPUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  D  E  OUTPUT\n",
       "0  1  1  0  0  0       1\n",
       "1  1  1  0  0  1       1\n",
       "2  0  0  0  0  0       0\n",
       "3  0  0  0  0  1       0\n",
       "4  1  1  0  1  0       1\n",
       "5  1  1  0  1  1       1\n",
       "6  1  1  1  1  0       1\n",
       "7  1  1  1  1  1       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv(r'test.csv', header = 0)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4dc7132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6220 - accuracy: 0.7500\n",
      "Accuracy: 75.00\n"
     ]
    }
   ],
   "source": [
    "test_arr = test.values\n",
    "test_input = test_arr[:,0:5]\n",
    "test_output = test_arr[:,5]\n",
    "\n",
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(test_input, test_output)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d307e0",
   "metadata": {},
   "source": [
    "How many hidden layers have you used? And why?\n",
    " - We used 2 hidden layers.\n",
    " - The data set, that we have, contains only 5 features. Since it has fewer deatures, it is better to go with 1 or 2 hidden layer.\n",
    " - Also, we need to remember that using more hidden layer will increase the complexity of the model\n",
    " \n",
    "How many nodes in each hidden layer and why that number of nodes in particular?\n",
    " - the number of nodes in the input layer is the same as the number of features the data has. So in our case, we have 5 nodes in the input layer.\n",
    " - Since the data is in binary, we decided to use the 'binary_crossentropy' cassification. In this case we'll want to use 2 nodes for the output, but that's not necessary since one of them has all weights equal to 0 and the output will always equal to 0 aswell. So using 1 node for the output make more sense. \n",
    " - there are multiple way to find how many nodes to uses in eash hidden layer. The one we decided to go with is 'The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.'\n",
    " - 5*2/3 +1 = 4.33333 \n",
    " - we rounded down to 4 for the first hidden layer\n",
    " - for the second one hidden layer, we decided to go with 2 nodes, because the first node will connect with 2 nodes of the first hidden layer and the second node will connect with other 2 nodes of the first hidden layer\n",
    " \n",
    "What is the activation function that you used and why? Did you use the same activation function in all layers? Why?\n",
    " - for the output layer, we used Sigmoid. Since we are using only one node for the output layer for the reason we explain above, we used Sigmoid with the output node in the place of softmax, because it will update faster. \n",
    " - for the two hidden layer, we used ReLU, because is the defaul activation function for the hidden layer and also it is less susceptible to vanishing gradients that prevent deep models from being trained \n",
    " \n",
    "What learning algorithm did you use to train the neural net and why?\n",
    "- for our datase, it make sense to chose between 'binary classification' and 'multiclass classification'. Since both will result with the same outcome of 1 or 0 it is better to go with 'binary classification'. \n",
    "\n",
    "Can you use one hidden layer only to solve this problem? If yes, how many nodes are you going to have in it? And why?\n",
    "- yes. Since our dataset is really a simple one with only 5 features, there won't be any problem by using only one hidden layer. The number of nodes to be used in this hidden layer will be the mean between the input layer and the output layer. \n",
    "\n",
    "- input layer = 5\n",
    "- output layer = 1\n",
    "- 5+1 = 6\n",
    "- 6/2 = 3\n",
    " \n",
    " - The number of nodes for this hidden layer will be 3\n",
    " - Using less nodes will lead to underfitting and using more nodes will lead to overfitting\n",
    "\n",
    "Can we use 5 hidden layers? Is that a good idea? Justify your answer.\n",
    "- It is possible to use 5 hidden layers, but it will not be a good idea. Using more then necessary hidden layer will cause the model to overfitting with the trainning dataset. This will cause the accuracy of the test dataset to decrease, because the model is overfitted it won't be able to generalize the new unseen data. \n",
    "\n",
    "How did the neural net do in classifying the testing set? Comment on how good or bad it learned the function from the training set.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb983510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
